[#chapter06-improve]
== Pérennisation de l’application

Il est capital de maintenir une certaine qualité des services. Ainsi dans ce chapitre je vais évoquer les actions que j'ai effectuées afin de mettre l'application en conformité avec les lois autour de la régulation des données. Je vais aussi parler de certaines actions que nous avons menées afin d'améliorer notre application.

=== RGPD

2018 fut l’année de la mise en place du règlement général sur la protection des données (RGPD). Entré en vigueur le 25 mai 2018, ce texte réglementaire européen couvre, entre autres, le traitement des données personnelles, c'est à dire toutes les données qui permettent d’identifier quelqu’un.

La réglementation impose qu’il y ait un https://www.cnil.fr/fr/devenir-delegue-la-protection-des-donnees[Délégué à la Protection des Données]. La https://www.cnil.fr/[Commission nationale de l’informatique et des Libertés (CNIL)] précise que le délégué doit s'assurer que la société respecte bien la réglementation applicable à leur protection. J’ai donc choisi de me porter volontaire pour ce rôle (Voir annexe "Désignation d'un délégué à la protection des données")

En me renseignant, j'ai donc noté que les principales nouveautés qu’apportent la RGPD sont:

* le droit à l’*oubli*
* le droit à la *portabilité des données*
* le droit à l’*opposition* de toute opération marketing
* le droit à la *rectification* des données de l’utilisateur
* le droit d’*accès* à toutes les données de l’utilisateur

Dans mon cas, les principales obligations qui m’incombent sont que l'utilisateur puisse:

* effacer toutes ses données
* exporter ses données

==== Le droit à la portabilité et à l'accès des données

Afin d'avoir un format normalisé et simple, j'ai choisi d'utiliser le format JSON et plus particulièrement la spécification https://jsonapi.org/[JSON:API] qui impose une structure du JSON.

En utilisant la https://github.com/Netflix/fast_jsonapi[librairie de serialization des données de Netflix], j'ai ainsi converti le modèle `User` au format JSON en incluant toutes ses liaisons. Ainsi, en un clic, l'utilisateur peut obtenir toutes ses données en incluant ses actes, ses messages et les significations qu'il a effectué.

==== Le droit à l’oubli

Le droit à l'oubli est l'une des plus grosse fonctionnalité demandée par la CNIL. https://gdpr-info.eu/art-17-gdpr/[L’article 17 de la RGPD] nous oblige à implémenter une fonctionnalité qui doit permettre à l’utilisateur d’effacer toutes ses données rapidement. La ou cela se complique, c’est que la RGPD nous oblige aussi à demander la suppression des données chez des fournisseurs tiers (comme Stripe dans mon cas).

NOTE: La fonctionnalité ne doit pas forcément entraîner un traitement automatisé, il peut simplement faire la demande qui sera effectué manuellement.

La difficulté de cette fonctionnalité est de supprimer les données liées à l’utilisateur sans impacter les données qui sont liées aux autres utilisateurs. Concrètement, cela signifie que lorsqu’un huissier de justice décide de supprimer son compte, il faut:

* supprimer son compte et ses données
* supprimer toutes ses factures
* supprimer les significations qu’il a effectué
* supprimer ses données chez Stripe

Cependant je ne peux pas supprimer les significations puisque le demandeur veut pouvoir les conserver. C’est pour cela que j’ai préféré l’*anonymisation* des données.

Étant un fan du _Test Driven Development_, j’ai commencé par construire les tests.

[source, ruby]
.test/models/bailiff_test.rb
----
test 'should anonymize and not destroy record' do
  old_user = @bailiff.as_json

  # on vérifie que l'huissier n'est pas supprime en base
  assert_no_difference('Bailiff.count') { @bailiff.destroy }

  # on vérifie que tous les champs suivants on ete modifies
  %i[email firstname lastname ....].each do |field|
    assert_not_equal old_user[field], @bailiff.send(field)
  end

  assert @bailiff.deleted
end
----

J’ai donc simplement surchargé la méthode `User#destroy` et utilisé la librairie `Faker` pour générer des données factices:

[source, ruby]
.app/models/user.rb
----
def destroy
  # ...
  #
  if customer = Stripe::Customer.retrieve(stripe_token)
      customer.delete
  end

  update_attributes(
    email: new_email,
    firstname: Faker::Name.first_name,
    lastname: Faker::Name.last_name,
    siret: Faker::Company.french_siret_number,
    # ...
    deleted: 1
  )
  save
end
----

Et voilà, la mise en conformité est faite. L’huissier ou l’avocat pourra supprimer son compte et les données seront conservées. En revanche, il sera impossible de retrouver l’utilisateur correspondant à ces données.

En plus de cela, j'ai aussi utilisé l'API de Stripe pour supprimer les données de nos utilisateur dans la base de données de Stripe lors de la suppression. Je ne le détaillerai pas la mise en place ici.


=== Déploiement continu

Afin d’être le plus réactif possible, j’ai voulu mettre en place un outil afin d’automatiser les mises en production. Cela est un énorme avantage car il permet de mettre en ligne un correctif ou une évolution très rapidement. De plus, automatiser permet de *réduire les risque* d’erreurs humaines.

.Un exemple d'erreur humaine qui peut coûter cher
image:reddit_rm_rf.png[]


J’ai donc utilisé la librairie https://capistranorb.com[Capistrano]. Il s'agit d'un outil écrit en Ruby qui permet d’automatiser le déploiement en s’appuyant sur Git (section <<gitflow>>). Il va donc:

* *Cloner* le répertoire Git afin de récupérer la dernière version du code source
* Lancer les *migrations* sur la base de données
* *Minifier* , c'est à dire concaténer les fichiers texte en un seul afin de réduire le nombre de requête HTTP et d’améliorer la vitesse de chargement] les fichiers JavaScript et CSS
* Faire des liens symboliques sur les documents que les utilisateurs ont envoyés sur le serveur

La librairie se met en place très facilement, voici un extrait de ma configuration

[source, ruby]
.config/deploy.rb
----
set :application, "iSignif"
set :repo_url, "http://git.rousseau-alexandre.fr/iSignif/Website.git"
append :linked_files, 'config/database.yml' , 'config/initializers/secret_token.rb', 'config/secrets.yml'
append :linked_dirs, 'public/uploads'
----

Une fois la librairie mise en place, il suffit d'utiliser la commande `cap production deploy` qui va s'occuper de mettre à jour l'application sur le serveur.

Cette approche m'a permis de faire des mise à jour de l'application plusieurs fois par semaines.

=== Sauvegarde Automatique du serveur

En terme de sauvegarde il existe plusieurs stratégies. Pour ma part, les conditions étaient d’avoir des sauvegardes qui inclue les donnée de MariaDB et les fichiers téléchargés sur le serveur par les utilisateurs.

De mon point de vue, la sauvegarde automatique doit être:

* *régulière et automatique*: j’ai donc utilisé une `crontab` qui est un utilitaire sous Linux qui permet de lancer des commandes à intervalles définis
* *chiffrée*: j’ai donc utilisé http://www.gnupg.org/[GNUPG] qui est un outil de chiffrement asymétrique (Méthode de chiffrement utilisant une paire de clés pour le cryptage). Ainsi, lorsque je chiffre une archive avec ma clé publique, seul mon PC distant peu l’ouvrir.

Voici donc une partie de mon script sur le listing suivant:

[source,bash]
.backup_server.sh
----
#!/bin/bash
# ...
# je réalise un dump de toute la BDD mariadb
mysqldump --all-databases  > $sqldump_filename
cp -r "/var/www/isignif/shared/{config,public,storage}" "${website}/shared/"
# je crée une archive et je la chiffre avec GPG
tar -czvf -  "$folder_save" | gpg --encrypt --recipient contact@rousseau-alexandre.fr -o "$folder_save.tar.gz.gpg"
# ...
----

Ensuite, un `cron` récupère régulièrement mes sauvegarde en utilisant un outil appelé https://fr.wikipedia.org/wiki/Rsync[Rsync].

NOTE: Il s'est avéré bien plus tard qu'une sauvegarde nous a permis de retrouver les données supprimées par accident par un de mes associé. Il avait supprimé une catégorie d'acte qui avait provoqué la suppression de plus de trente actes avec toutes les données liées (messages, fichiers, etc..).

[#improve_roles]
=== Gestion des rôles pour les utilisateurs

Un commercial a rejoint notre équipe. Cela a mené à une  problématique car il devait avoir accès aux données de ses clients afin de les aider mais nous ne voulions pas qu'il puisse les modifier.

J'ai  donc choisis de définir des rôles afin de limiter les actions possibles sur l'application. Voici la liste des rôles que j'ai défini:

- `user`: utilisateur classique
- `support`: a accès aux entités des autres utilisateurs (actes, messagerie, etc..) et à leurs tableaux de bord
- `admin`: peut clôturer des actes et supprimer des éléments à faible impact
- `god` : peut tout faire

J'ai ainsi choisi d'utiliser la libraire https://github.com/varvet/pundit[Pundit] qui permet la mise en place de _policies_. Les _policies_ sont des règles d'accès aux actions définies par les contrôleurs. Voici un exemple:

[source,ruby]
.app/policies/act_family_policy.rb
----
class ActFamilyPolicy < ApplicationPolicy

  def show?
    logged? && (@user.support? || @user.god?)
  end

  def destroy?
    logged? && @user.god?
  end

  # ...
----

Cette _policie_ définit que:

- seuls les utilisateurs de type `support` ou `god` peuvent consulter les `ActFamily`
- seuls les utilisateurs de type `god` peuvent supprimer les `ActFamily`


En faisant cela je m'assure de restreindre les "accès dangereux" aux utilisateurs qui en ont vraiment besoin.

=== Suivis des améliorations

Afin de suivre les amélioration des fonctionnalités, je met à jour un fichier YAML qui note pour chaque version:

- la date de mise en production
- les nouveautés
- les corrections

Ce fichier, versionné avec le code source de l'application, permet à mes associés de savoir en temps réel et de communiquer autour des modifications effectuées sur l'application.

.Capture d'écran des notes de version sur isignif.fr
image:changelog.png[]


=== Conclusion

L'amélioration du produit est une étape importante car elle permet de *fiabiliser* le produit. Cette fiabilisation fidélise le client qui est rassuré par cette qualité.

Si j'avais omis cette étape en privilégiant le développement d'une nouvelle fonctionnalité, nous aurions été décrédibilisé lors de notre première perte des données. Je peux donc affirmer que cette étape a été une *étape clé* de iSignif.

Je regrette simplement de ne pas avoir mis en place un *serveur d'intégration continue* qui lance les tests unitaires et scanne le code. En effet, cette étape est manuelle et, comme je l'ai expliqué plus haut, une action manuelle laisse place à l'erreur.
